#----------------------
#
#
#----------------------

import numpy as np
import pandas as pd
import plotly.express as px
import os 
from sklearn.cluster import DBSCAN, AgglomerativeClustering
from sklearn.preprocessing import StandardScaler
from sklearn.manifold import Isomap
import umap.umap_ as umap
from PIL import Image
from sklearn.utils import shuffle



path_features = "C:/xc_real_projects/features"

# imgpath = "C:/xc_real_projects/xc_aec_project_sw_europe/downloaded_data_img_24000sps"
imgpath = "C:/xc_real_projects/xc_streamlit_sw_eur/downloaded_data_img_24000sps"

path_clust_images = "C:/xc_real_projects/clusters"

path_save = os.path.join(path_features, 'features_medium20250315_235946.npz')

#-------------------------
# parame Reduce dimensionality
n_dims_red = 32 # 32 seems goooood 
# n_dims_red = 64 # 16 # 16 or 32 are ok
# n_neighbors = 10 # between 5-15 seems ok
n_neighbors = 15 # between 5-15 seems ok

eps_search = np.arange(0.03, 0.8, 0.03) # defaults 32
# eps_search = np.arange(0.30, 0.70, 0.05) # dim  64
# eps_search = np.arange(0.10, 3.60, 0.20) # large  256
# eps_search = np.arange(14, 19, 0.50) # super large  1500







#-------------------------
# (x) Load AEC features 
#-------------------------

data = np.load(file = path_save)
feat    = data['feat']
imfiles = data['imfiles']
feat.shape
imfiles.shape

# shuffle
feat, imfiles = shuffle(feat, imfiles)
feat.shape
imfiles.shape


#-------------------------
# apply slidine witndow 
#-------------------------

num_files = imfiles.shape[0]

n_feats_per_clip = feat.shape[2]

moving_wind_siz = 8

feat = np.lib.stride_tricks.sliding_window_view(feat, window_shape = (moving_wind_siz,), axis=2)
feat.shape
feat = np.moveaxis(feat, 2, 1)
feat.shape
the_multiplier = feat.shape[1]
feat = feat.reshape(feat.shape[0]*feat.shape[1], feat.shape[2], feat.shape[3])
feat.shape


imfiles.shape
imfiles = np.repeat(a = imfiles, repeats = the_multiplier)
imfiles.shape


time_position = (((np.arange(the_multiplier))/the_multiplier) * n_feats_per_clip).astype(int)
time_position = np.concatenate([time_position]*num_files)
time_position.shape


# check
feat.shape
imfiles.shape
time_position.shape







# subsamples 
feat.shape
imfiles.shape
time_position.shape


feat = feat[::4]
imfiles         = imfiles[::4]
time_position   = time_position[::4]


# check
feat.shape
imfiles.shape
time_position.shape








# take a pooling function over time to get classic feature vector 
feature_mat = np.concatenate([feat.max(2),  feat.mean(2) , feat.std(2)], axis = 1)
# feature_mat = feat.max(2)
# feature_mat = feat.mean(2)
feature_mat.shape

# select valid features 
feature_mat.std(0).shape
sel_feats = feature_mat.std(0) != 0.000 # or feature_mat.std(0) != 0.0
feature_mat_red = feature_mat[:,sel_feats]
feature_mat_red.shape







#-------------------------
# (x) standardize 1
#-------------------------

scaler = StandardScaler()
scaler.fit(feature_mat_red)
feature_mat_scaled_1 = scaler.transform(feature_mat_red)
# feature_mat_scaled_1.mean(0)
# feature_mat_scaled_1.std(0)





#-------------------------
# (x) Reduce dimensionality
#-------------------------
# 
if True:
    reducer = umap.UMAP(n_neighbors=n_neighbors, n_components=n_dims_red, metric = 'euclidean')
    # reducer =    Isomap(n_neighbors=n_neighbors, n_components=n_dims_red, metric = "euclidean")
    reducer.fit(feature_mat_scaled_1[0:10000])
    X_trans = reducer.transform(feature_mat_scaled_1)
    X_trans.shape



#-------------------------
# (x) standardize 2
#-------------------------

scaler = StandardScaler()
scaler.fit(X_trans)
feature_mat_scaled_2 = scaler.transform(X_trans)
feature_mat_scaled_2.shape

# fig01 = px.scatter(
#     x = feature_mat_scaled_2[:,0],
#     y = feature_mat_scaled_2[:,1]
#     )
# fig01.show()




# # save smaller data just befor clustering 

# imfiles.shape
# feature_mat_scaled_2.shape
# imfiles.dtype
# feature_mat_scaled_2.dtype

# path_save_mini = os.path.join(path_features, 'features_reduced_99.npz')
# np.savez(file = path_save_mini, feat=feature_mat_scaled_2, imfiles=imfiles)





#-------------------------
# (x) Clustering
#-------------------------

for eps_i in eps_search:
    print("-----------")
    print(">> eps_i", eps_i)
    clu = DBSCAN(eps = eps_i, min_samples=10, metric='euclidean', n_jobs = 4) 
    cluster_ids = clu.fit_predict(feature_mat_scaled_2)
    pd.Series(cluster_ids).value_counts()[0:10]
 
    # select only large enough clusters 
    sel0 = pd.Series(cluster_ids).value_counts() > 10
    sel1 = pd.Series(cluster_ids).value_counts() < 500
    sel = np.logical_and(sel0, sel1)
    sel2 = pd.Series(cluster_ids).value_counts().loc[sel].index
    cluster_ids.shape

    try:
        cluster_ids_sel    = cluster_ids[pd.Series(cluster_ids).isin(sel2)]
        finle_name_arr_sel = imfiles[pd.Series(cluster_ids).isin(sel2)]
        time_position_sel  = time_position[pd.Series(cluster_ids).isin(sel2)]

        cluster_ids_sel.shape
        finle_name_arr_sel.shape

        # prepare df 
        df = pd.DataFrame({
            'file_name' :finle_name_arr_sel,
            'cluster_id' :cluster_ids_sel,
            'time_position' :time_position_sel,
            })
        df['newname'] = df['cluster_id'].astype(str).str.cat(others=df['file_name'], sep='_')
        df = df.sort_values(by = 'cluster_id')
        df.shape

        # MAKE NICE MOSAIK PLOT 
        imall = Image.new('L', (20000, 10000), '#55ff00')
        gap = 25
        current_id = df['cluster_id'].min()
        vert_counter = gap
        horiz_counter = gap
        for i,r in df.iterrows():
            im = Image.open(os.path.join(imgpath, r['file_name']))

            # tak only relevan part of image  in time
            w, h = im.size
            c1 = r['time_position']
            c2 = r['time_position']+128

            # im.crop((left, top, right, bottom))

            im = im.crop((c1, 0, c2, h))
            # im.size

            if r['cluster_id'] > current_id: 
                vert_counter = vert_counter + 128 + gap
                horiz_counter = 0 + gap
                current_id = r['cluster_id']
            imall.paste(im, ( horiz_counter, vert_counter))
            horiz_counter = horiz_counter + 128 + gap     
        # imall.show()

        finam = 'neigh_' + "{:1.0f}".format(n_neighbors)  + '_dims_' + "{:1.0f}".format(n_dims_red)  + '_eps_' +   "{:5.4f}".format(eps_i) 
        imall.save(os.path.join(path_clust_images,  finam + ".png" ))
        df.to_pickle(path = os.path.join(path_clust_images,  finam + ".pkl" )  )

    except:
        print("haha")











#----------------------
#
#
#----------------------

import numpy as np
import pandas as pd
import plotly.express as px
import os 
from sklearn.cluster import DBSCAN, AgglomerativeClustering
from sklearn.preprocessing import StandardScaler
from sklearn.manifold import Isomap
import umap.umap_ as umap
from PIL import Image
from sklearn.utils import shuffle



path_features = "C:/xc_real_projects/features"

imgpath = "C:/xc_real_projects/xc_aec_project_sw_europe/downloaded_data_img_24000sps"

path_clust_images = "C:/xc_real_projects/clusters"

path_save = os.path.join(path_features, 'features_medium20250313_185849.npz')


#-------------------------
# parame Reduce dimensionality
n_dims_red = 32 # 32 seems goooood 
# n_dims_red = 64 # 16 # 16 or 32 are ok
n_neighbors = 10 # between 5-15 seems ok
eps_search = np.arange(0.03, 0.8, 0.03) # defaults 32
# eps_search = np.arange(0.30, 0.70, 0.05) # dim  64
# eps_search = np.arange(0.10, 3.60, 0.20) # large  256
# eps_search = np.arange(14, 19, 0.50) # super large  1500







#-------------------------
# (x) Load AEC features 
#-------------------------

data = np.load(file = path_save)
feat    = data['feat']
imfiles = data['imfiles']
feat.shape
imfiles.shape
# feat = feat[0:100000,:]
# imfiles = imfiles[0:100000]
feat.shape
imfiles.shape



# shuffle
feat, imfiles = shuffle(feat, imfiles)
feat.shape
imfiles.shape


#-------------------------
# (x)
#-------------------------

# atak only center time bins 
feat.shape
feat = feat[:, :,1:7]
feat.shape




# # 
# # select only where average feature value high
# per_image_mean = feat.mean((1,2))
# per_image_mean.shape
# pimm = per_image_mean.mean()
# feat = feat[per_image_mean > pimm]
# feat.shape
# imfiles = imfiles[per_image_mean > pimm]
# imfiles.shape






# take a pooling function over time to get classic feature vector 
# feature_mat = np.concatenate([feat.max(2),  feat.mean(2) ], axis = 1)
feature_mat = np.concatenate([feat.max(2),  feat.mean(2) , feat.std(2)], axis = 1)
# feature_mat = np.concatenate([feat.mean(2) , feat.std(2)], axis = 1)

# feature_mat = feat.max(2)
# feature_mat = feat.mean(2)
feature_mat.shape

# select valid features 
feature_mat.std(0).shape
sel_feats = feature_mat.std(0) != 0.000 # or feature_mat.std(0) != 0.0
feature_mat_red = feature_mat[:,sel_feats]
feature_mat_red.shape


# check features's univariate distribution
if False: 
    XX = feature_mat_red[0:2000, :]
    jit = np.random.uniform(size=XX.shape[0])
    for ii in np.random.randint(0, XX.shape[1], size = 3):
        print(ii)
        xx = XX[:,ii]
        figa = px.scatter(x = xx, y = jit)
        figa.show()

if False: 
# look for correlations 
    XX = pd.DataFrame(feature_mat_red)
    XX.shape
    corr_mat = XX.corr()
    corr_mat = corr_mat.values
    type(corr_mat)
    np.fill_diagonal(a = corr_mat, val = 0.0)
    np.diag(corr_mat)
    corr_mat.max()
    corr_mat.min()
    u = (np.tril(corr_mat, 0)).flatten()
    u.shape
    np.sort(u) 
    jit = np.random.uniform(size=u.shape[0])
    figa = px.scatter(x = u, y = jit)
    figa.show()





#-------------------------
# (x) standardize 1
#-------------------------

scaler = StandardScaler()
scaler.fit(feature_mat_red)
feature_mat_scaled_1 = scaler.transform(feature_mat_red)
# feature_mat_scaled_1.mean(0)
# feature_mat_scaled_1.std(0)





#-------------------------
# (x) Reduce dimensionality
#-------------------------
# 
if True:
    reducer = umap.UMAP(n_neighbors=n_neighbors, n_components=n_dims_red, metric = 'euclidean')
    # reducer =    Isomap(n_neighbors=n_neighbors, n_components=n_dims_red, metric = "euclidean")
    reducer.fit(feature_mat_scaled_1[0:7000])
    X_trans = reducer.transform(feature_mat_scaled_1)
    X_trans.shape

# skip umap    
if False:
    X_trans = feature_mat_scaled_1
    X_trans.shape

#-------------------------
# (x) standardize 2
#-------------------------

scaler = StandardScaler()
scaler.fit(X_trans)
feature_mat_scaled_2 = scaler.transform(X_trans)
feature_mat_scaled_2.shape

# fig01 = px.scatter(
#     x = feature_mat_scaled_2[:,0],
#     y = feature_mat_scaled_2[:,1]
#     )
# fig01.show()




# save smaller data just befor clustering 

imfiles.shape
feature_mat_scaled_2.shape
imfiles.dtype
feature_mat_scaled_2.dtype

path_save_mini = os.path.join(path_features, 'features_reduced_2.npz')
np.savez(file = path_save_mini, feat=feature_mat_scaled_2, imfiles=imfiles)





#-------------------------
# (x) Clustering
#-------------------------

for eps_i in eps_search:
    print("-----------")
    print(">> eps_i", eps_i)
    clu = DBSCAN(eps = eps_i, min_samples=10, metric='euclidean', n_jobs = 4) 
    cluster_ids = clu.fit_predict(feature_mat_scaled_2)
    pd.Series(cluster_ids).value_counts()[0:10]
 
    # select only large enough clusters 
    sel0 = pd.Series(cluster_ids).value_counts() > 10
    sel1 = pd.Series(cluster_ids).value_counts() < 500
    sel = np.logical_and(sel0, sel1)
    sel2 = pd.Series(cluster_ids).value_counts().loc[sel].index
    cluster_ids.shape

    try:
        cluster_ids_sel    = cluster_ids[pd.Series(cluster_ids).isin(sel2)]
        finle_name_arr_sel = imfiles[pd.Series(cluster_ids).isin(sel2)]
        cluster_ids_sel.shape
        finle_name_arr_sel.shape

        # prepare df 
        df = pd.DataFrame({
            'file_name' :finle_name_arr_sel,
            'cluster_id' :cluster_ids_sel,
            })
        df['newname'] = df['cluster_id'].astype(str).str.cat(others=df['file_name'], sep='_')
        df = df.sort_values(by = 'cluster_id')
        df.shape

        # MAKE NICE MOSAIK PLOT 
        imall = Image.new('L', (20000, 10000), '#55ff00')
        gap = 25
        current_id = df['cluster_id'].min()
        vert_counter = gap
        horiz_counter = gap
        for i,r in df.iterrows():
            im = Image.open(os.path.join(imgpath, r['file_name']))
            if r['cluster_id'] > current_id: 
                vert_counter = vert_counter + 128 + gap
                horiz_counter = 0 + gap
                current_id = r['cluster_id']
            imall.paste(im, ( horiz_counter, vert_counter))
            horiz_counter = horiz_counter + 128 + gap     
        # imall.show()

        finam = 'neigh_' + "{:1.0f}".format(n_neighbors)  + '_dims_' + "{:1.0f}".format(n_dims_red)  + '_eps_' +   "{:5.4f}".format(eps_i) 
        imall.save(os.path.join(path_clust_images,  finam + ".png" ))
        df.to_pickle(path = os.path.join(path_clust_images,  finam + ".pkl" )  )

    except:
        print("haha")











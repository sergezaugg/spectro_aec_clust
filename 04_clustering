#----------------------
#
#
#----------------------

import numpy as np
import pandas as pd
import plotly.express as px
import os 
import shutil
from sklearn.cluster import DBSCAN, AgglomerativeClustering
from sklearn.preprocessing import StandardScaler
import pickle


path_features = "C:/xc_real_projects/features"

imgpath = "C:/xc_real_projects/xc_aec_project_sw_europe/downloaded_data_img_24000sps"

path_clust_images = "C:/xc_real_projects/clusters"





path_save = os.path.join(path_features, 'features_20250306_104526.npz')


data = np.load(file = path_save)

feat    = data['feat']
imfiles = data['imfiles']

feat.shape
imfiles.shape



# take a pooling function over time to get classic feature vector 
feat_max = feat.max(2)
feat_mea = feat.mean(2)
feat_max.shape
feat_mea.shape


# rename
sel_subset = 20000


# feature_mat = feat_max[0:sel_subset]
feature_mat = feat_mea[0:sel_subset]


finle_name_arr = imfiles[0:sel_subset]
feature_mat.shape



# select valid features 
feature_mat.std(0).shape
sel_feats = feature_mat.std(0) != 0.000 # or feature_mat.std(0) != 0.0
sel_feats.sum()
feature_mat_red = feature_mat[:,sel_feats]
feature_mat_red.shape

# log transform 
# check features's univariate distribution
if False: 
    XX = feature_mat_red[0:2000, :]
    jit = np.random.uniform(size=XX.shape[0])
    for ii in np.random.randint(0, XX.shape[1], size = 3):
        print(ii)
        xx = XX[:,ii]
        figa = px.scatter(x = xx, y = jit)
        figa.show()


if False: 
# look for correlations 
    XX = pd.DataFrame(feature_mat_red)
    XX.shape
    corr_mat = XX.corr()
    corr_mat = corr_mat.values
    type(corr_mat)
    np.fill_diagonal(a = corr_mat, val = 0.0)
    np.diag(corr_mat)
    corr_mat.max()
    corr_mat.min()
    u = (np.tril(corr_mat, 0)).flatten()
    u.shape
    np.sort(u) 
    jit = np.random.uniform(size=u.shape[0])
    figa = px.scatter(x = u, y = jit)
    figa.show()








# standardize 
scaler = StandardScaler()
scaler.fit(feature_mat_red)
feature_mat_scaled = scaler.transform(feature_mat_red)
# feature_mat_scaled.shape
# feature_mat_scaled.mean(0).shape
# feature_mat_scaled.mean(0)
# feature_mat_scaled.std(0)




from sklearn.manifold import Isomap, TSNE


feature_mat_scaled.shape

embedding = Isomap(n_neighbors=10, n_components=16, metric = "euclidean")

embedding.fit(feature_mat_scaled[0:4000])

X_trans = embedding.transform(feature_mat_scaled)

X_trans.shape








# # clustering 
# clu = AgglomerativeClustering(n_clusters=1000, metric='euclidean', linkage='average')
# # cluster_ids = clu.fit_predict(feature_mat_scaled)
# cluster_ids = clu.fit_predict(X_trans)
# # cluster_ids.shape
# pd.Series(cluster_ids).value_counts()




for eps_i in np.arange(1,20, 1):
# for eps_i in np.arange(0.5,10, 1):
    print("-----------")
    print(">> eps_i", eps_i)
    clu = DBSCAN(eps= eps_i, min_samples=5, metric='euclidean') # eps 10.5 11.0 good min_samples=10
    cluster_ids = clu.fit_predict(X_trans)
    cluster_ids = cluster_ids[cluster_ids != -1]
    pd.Series(cluster_ids).value_counts()[0:10]
    print("")
  

clu = DBSCAN(eps = 5, min_samples=5, metric='euclidean') # eps 10.5 11.0 good min_samples=10
cluster_ids = clu.fit_predict(X_trans)
cluster_ids.shape
pd.Series(cluster_ids).value_counts()[0:20]




# select only large enough clusters 
sel0 = pd.Series(cluster_ids).value_counts() > 8
sel1 = pd.Series(cluster_ids).value_counts() < 500
sel = np.logical_and(sel0, sel1)

sel2 = pd.Series(cluster_ids).value_counts().loc[sel].index
cluster_ids.shape
sel.shape

cluster_ids_sel = cluster_ids[ pd.Series(cluster_ids).isin(sel2)]
finle_name_arr_sel = finle_name_arr[ pd.Series(cluster_ids).isin(sel2)]
cluster_ids_sel.shape
finle_name_arr_sel.shape



# save images by cluster id 
df = pd.DataFrame({
    'file_name' :finle_name_arr_sel,
    'cluster_id' :cluster_ids_sel,
    })
df['newname'] = df['cluster_id'].astype(str).str.cat(others=df['file_name'], sep='_')


for i,r in df.iterrows():
    # print(r)
    # if r['cluster_id'] == -1:
    #     continue
    # if r['cluster_id'] == 0:
    #     continue
    print(r['cluster_id'])
    path_cli =  os.path.join(path_clust_images, str(r['cluster_id']))
    if not os.path.exists(path_cli):
        os.mkdir(path_cli)
    src = os.path.join(imgpath, r['file_name'])
    dst = os.path.join(path_cli, r['newname'])
    shutil.copy(src, dst)



















# from sklearn.manifold import Isomap, TSNE

# X = feature_mat_scaled
# X.shape

# # embedding = Isomap(n_neighbors=10, n_components=2, metric = "l1")
# # embedding.fit(X[:3000])
# # X_trans = embedding.transform(X[:10000])
# # X_trans.shape

# # 30
# embedding = TSNE(n_components=2, perplexity=100.0, metric='l1')
# X_trans = embedding.fit_transform(X[:5000])
# fi_trans = finle_name_arr[:5000]


# fig00 = px.scatter(
#     x = X_trans[:,0],
#     y = X_trans[:,1]
#     )

# fig00.show()



# X_trans.shape

# fi_trans.shape



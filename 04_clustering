#----------------------
#
#
#----------------------

import numpy as np
import pandas as pd
import plotly.express as px
import os 
import shutil
from sklearn.cluster import DBSCAN, AgglomerativeClustering
from sklearn.preprocessing import StandardScaler
import pickle


path_features = "C:/xc_real_projects/features"

imgpath = "C:/xc_real_projects/xc_aec_project_sw_europe/downloaded_data_img_24000sps"

path_clust_images = "C:/xc_real_projects/clusters"


tstmp = "20250304_182800"

path_save = os.path.join(path_features, 'features_' + tstmp + '.npz')


data = np.load(file = path_save)

feat    = data['feat']
imfiles = data['imfiles']

feat.shape
imfiles.shape



# take a pooling function over time to get classic feature vector 
feat_max = feat.max(2)
feat_mea = feat.mean(2)
feat_max.shape
feat_mea.shape


# rename
sel_subset = 15000
feature_mat = feat_max[0:sel_subset]
# feature_mat = feat_mea[0:sel_subset]
finle_name_arr = imfiles[0:sel_subset]
feature_mat.shape



# select valid features 
feature_mat.std(0).shape
sel_feats = feature_mat.std(0) != 0.000 # or feature_mat.std(0) != 0.0
sel_feats.sum()
feature_mat_red = feature_mat[:,sel_feats]
feature_mat_red.shape

# standardize 
scaler = StandardScaler()
scaler.fit(feature_mat_red)
feature_mat_scaled = scaler.transform(feature_mat_red)
# feature_mat_scaled.shape
# feature_mat_scaled.mean(0).shape
# feature_mat_scaled.mean(0)
# feature_mat_scaled.std(0)








# clustering 
clu = AgglomerativeClustering(n_clusters=400, metric='euclidean', linkage='complete')
cluster_ids = clu.fit_predict(feature_mat_scaled)
# cluster_ids.shape
pd.Series(cluster_ids).value_counts()





# select only large enough clusters 
sel0 = pd.Series(cluster_ids).value_counts() > 20
sel1 = pd.Series(cluster_ids).value_counts() < 500
sel = np.logical_and(sel0, sel1)

sel2 = pd.Series(cluster_ids).value_counts().loc[sel].index
cluster_ids.shape
sel.shape

cluster_ids_sel = cluster_ids[ pd.Series(cluster_ids).isin(sel2)]
finle_name_arr_sel = finle_name_arr[ pd.Series(cluster_ids).isin(sel2)]
cluster_ids_sel.shape
finle_name_arr_sel.shape



# save images by cluster id 
df = pd.DataFrame({
    'file_name' :finle_name_arr_sel,
    'cluster_id' :cluster_ids_sel,
    })
df['newname'] = df['cluster_id'].astype(str).str.cat(others=df['file_name'], sep='_')


for i,r in df.iterrows():
    # print(r)
    if r['cluster_id'] == -1:
        continue
    if r['cluster_id'] == 13:
        continue
    print(r['cluster_id'])
    path_cli =  os.path.join(path_clust_images, str(r['cluster_id']))
    if not os.path.exists(path_cli):
        os.mkdir(path_cli)
    src = os.path.join(imgpath, r['file_name'])
    dst = os.path.join(path_cli, r['newname'])
    shutil.copy(src, dst)















# # for eps_i in np.arange(1,50, 5):
# for eps_i in np.arange(17,20, 0.5):
#     print("-----------")
#     print(">> eps_i", eps_i)
#     clu = DBSCAN(eps= eps_i, min_samples=8, metric='euclidean') # eps 10.5 11.0 good min_samples=10
#     cluster_ids = clu.fit_predict(feat_temp)
#     cluster_ids = cluster_ids[cluster_ids != -1]
#     pd.Series(cluster_ids).value_counts()[0:10]
#     print("")
#     print("")
#     print("")


# clu = DBSCAN(eps= 18.5, min_samples=8, metric='euclidean') # eps 10.5 11.0 good min_samples=10
# cluster_ids = clu.fit_predict(feature_mat_scaled)
# cluster_ids.shape
# pd.Series(cluster_ids).value_counts()[0:20]





from sklearn.manifold import Isomap, TSNE

X = feature_mat_scaled
X.shape

embedding = Isomap(n_neighbors=10, n_components=2, metric = "l1")
embedding.fit(X[:3000])
X_trans = embedding.transform(X[:10000])
X_trans.shape

# 30
embedding = TSNE(n_components=2, perplexity=100.0, metric='l1')
X_trans = embedding.fit_transform(X[:5000])

fig00 = px.scatter(
    x = X_trans[:,0],
    y = X_trans[:,1]
    )

fig00.show()
